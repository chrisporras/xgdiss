{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rU1nmWJG2FFe",
        "3jTooPyB40af"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisporras/xgdiss/blob/main/ChrisPorras_GradCAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project 2023\n",
        "## GradCAM plug n play\n",
        "\n",
        "Machine Learning for Biomedical Data Science\n",
        "\n",
        "Team Xtreme Gradient Dissenters\n",
        "\n",
        "Members: Audrey Lee, Christian Porras, Joy Jiang\n",
        "\n",
        "April 24, 2023"
      ],
      "metadata": {
        "id": "ATlDP5GeIKX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using explainable AI framework `OmniXAI`\n",
        "\n",
        "https://github.com/salesforce/OmniXAI"
      ],
      "metadata": {
        "id": "RfKUk9w8WKqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages"
      ],
      "metadata": {
        "id": "lSzVJZR42CFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q omnixai # explainable ai framework\n",
        "!pip install -q dash # web app, visualization\n",
        "!pip install -q jupyter-dash # dash + jupyter notebooks/colab\n",
        "!pip install -q dash_bootstrap_components # dash utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLwgpuHq2Ogn",
        "outputId": "df345203-0250-4f4b-cb93-7953803219af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m534.9/534.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m758.0/758.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.4/572.4 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.6/220.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEED TO RESTART RUNTIME AFTER INSTALLING PACKAGES\n",
        "import omnixai # WILL THROW ERROR IF RUNTIME NOT RESTARTED\n",
        "# USE THIS TO TEST PACKAGE INSTALL"
      ],
      "metadata": {
        "id": "pZhaZThLCakh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for saving output htmls & accessing model .pt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr_1Hy4eE80t",
        "outputId": "bb13682a-763b-4d43-9a43-2f9233344e62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "Run all cells to download data from kaggle and unzip into working directory."
      ],
      "metadata": {
        "id": "rU1nmWJG2FFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chrisporras/xgdiss.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIXUUlQOD8B4",
        "outputId": "2bfc7f9f-00fd-4d9d-9b58-5c3e98b96ff7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'xgdiss'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 58 (delta 20), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (58/58), 3.69 MiB | 7.76 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Kaggle public api\n",
        "! pip install -q kaggle\n",
        "# Choose the kaggle.json file that you downloaded\n",
        "! mkdir ~/.kaggle\n",
        "! cp ./xgdiss/kaggle.json ~/.kaggle/\n",
        "# Make directory named kaggle and copy kaggle.json file there.\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "#Change the permissions of the file.\n",
        "! kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd3at7OCI5uF",
        "outputId": "401e8732-16df-4918-ef23-a434ea3dc83a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                            title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "-------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "salvatorerastelli/spotify-and-youtube                          Spotify and Youtube                                   9MB  2023-03-20 15:43:25          10089        353  1.0              \n",
            "arnabchaki/data-science-salaries-2023                          Data Science Salaries 2023  💸                        25KB  2023-04-13 09:55:16           3853         89  1.0              \n",
            "erdemtaha/cancer-data                                          Cancer Data                                          49KB  2023-03-22 07:57:00           4856        102  1.0              \n",
            "evangower/premier-league-2022-2023                             Premier League 2022-2023                              7KB  2023-04-14 19:45:22            862         30  1.0              \n",
            "lokeshparab/amazon-products-dataset                            Amazon Products Sales Dataset 2023                   80MB  2023-03-26 10:45:19           5240        112  1.0              \n",
            "iammustafatz/diabetes-prediction-dataset                       Diabetes prediction dataset                         734KB  2023-04-08 06:11:45           1956         39  1.0              \n",
            "ulrikthygepedersen/fastfood-nutrition                          Fastfood Nutrition                                   12KB  2023-03-21 10:02:41           4202         78  1.0              \n",
            "mikoajfish99/us-recession-and-financial-indicators             🏛️  Financial Indicators of US Recession 📉          989KB  2023-04-17 13:54:47            581         28  1.0              \n",
            "desalegngeb/students-exam-scores                               Students Exam Scores: Extended Dataset              695KB  2023-04-14 00:15:38           1346         37  1.0              \n",
            "rkiattisak/student-performance-in-mathematics                  Student performance prediction                        9KB  2023-03-12 04:32:56          10078        217  1.0              \n",
            "ppb00x/credit-risk-customers                                   credit_risk_customers                                18KB  2023-04-12 08:28:28           1669         44  1.0              \n",
            "ritwikb3/heart-disease-cleveland                               Heart Disease Cleveland                               3KB  2023-03-28 17:48:48            746         25  1.0              \n",
            "ppb00x/country-gdp                                             Country_GDP                                           7KB  2023-04-07 06:47:36           1537         40  1.0              \n",
            "dansbecker/melbourne-housing-snapshot                          Melbourne Housing Snapshot                          451KB  2018-06-05 12:52:24         113634       1250  0.7058824        \n",
            "r1shabhgupta/google-stock-price-daily-weekly-and-monthly-2023  Google Stock Price: Daily, Weekly & Monthly (2023)   61KB  2023-04-15 18:23:08            799         31  1.0              \n",
            "omartorres25/honda-data                                        Honda Cars Data                                     184KB  2023-03-28 04:19:11           1761         35  1.0              \n",
            "harshghadiya/kidneystone                                       Kidney Stone Dataset                                  2KB  2023-04-12 06:09:00            924         24  0.8235294        \n",
            "ashishraut64/internet-users                                    Global Internet users                               163KB  2023-03-29 12:25:13           2799         67  1.0              \n",
            "andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews   🎬 Massive Rotten Tomatoes Movies & Reviews          152MB  2023-04-13 10:58:54            645         22  1.0              \n",
            "r1chardson/the-world-university-rankings-2011-2023             THE World University Rankings 2011-2023               1MB  2023-04-03 12:43:37           2691         60  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d nickuzmenkov/strip-ai-256x256-png-tiles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeWyZpT1QPUC",
        "outputId": "4025c275-2430-49bd-b556-b20b6fd7424e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading strip-ai-256x256-png-tiles.zip to /content\n",
            " 33% 685M/2.03G [00:33<00:58, 25.2MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip strip-ai-256x256-png-tiles.zip"
      ],
      "metadata": {
        "id": "3BHooDJcJB3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradCAM"
      ],
      "metadata": {
        "id": "UnxKTjfjZftf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explainer pipeline functions"
      ],
      "metadata": {
        "id": "3jTooPyB40af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _plotly_figure(self, index, class_names=None, **kwargs):\n",
        "    import plotly.express as px\n",
        "    values = self.results[\"values\"][index]\n",
        "    labels = self.results[\"labels\"]\n",
        "    if labels is None:\n",
        "        fnames, scores = [\"Predicted value\"], [values]\n",
        "    else:\n",
        "        fnames, scores = labels[index], values\n",
        "        fnames = [class_names[f] for f in fnames] \\\n",
        "            if class_names is not None else [str(f) for f in fnames]\n",
        "    fig = px.bar(\n",
        "        y=fnames[::-1],\n",
        "        x=scores[::-1],\n",
        "        orientation=\"h\",\n",
        "        labels={\"x\": \"Predicted value\",\n",
        "                \"y\": \"Label\" if labels is not None else \"Target\"},\n",
        "        title=\"\",\n",
        "        color_discrete_map={True: \"#008B8B\", False: \"#DC143C\"},\n",
        "    )\n",
        "    return fig"
      ],
      "metadata": {
        "id": "v4bTY-LdMMEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_gradcam(model, test, idx, outpath):\n",
        "  import json\n",
        "  import torch\n",
        "  from torchvision import models, transforms\n",
        "  from PIL import Image as PilImage\n",
        "  from omnixai.preprocessing.image import Resize\n",
        "  from omnixai.data.image import Image\n",
        "  from omnixai.explainers.vision import VisionExplainer\n",
        "  # from omnixai.visualization.dashboard import Dashboard\n",
        "  # In this example, we consider an image classification task. We recommend using `Image`\n",
        "  # to represent a batch of images. `Image` can be constructed from a numpy array or a Pillow\n",
        "  # image. The following code loads a test image and resizes it to (256, 256).\n",
        "  # img = Resize((256, 256)).transform(Image(PilImage.open('data/images/dog_cat.png').convert('RGB')))\n",
        "  img = Image(PilImage.open(test['file_path'][idx]).convert('RGB'))\n",
        "  # For visulization, the class names corresponding to the labels are also loaded.\n",
        "  # with open('data/images/imagenet_class_index.json', 'r') as read_file:\n",
        "  #     class_idx = json.load(read_file)\n",
        "  #     idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  # The preprocessing function takes an `Image` instance as its input and outputs the\n",
        "  # processed features that the ML model consumes. In this example, the `Image` object is\n",
        "  # first converted into a torch tensor via the defined transform and sent to particular\n",
        "  # device.\n",
        "  train_ds_mean = torch.tensor([0.9113, 0.8299, 0.8212])\n",
        "  train_ds_std = torch.tensor([0.1397, 0.2390, 0.3153])\n",
        "  transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Resize((224,224)),\n",
        "        transforms.Normalize(train_ds_mean, train_ds_std)\n",
        "  ])\n",
        "  # preprocess = lambda ims: torch.stack([transform(im.to_pil()) for im in ims])\n",
        "\n",
        "  # transform = transforms.Compose([\n",
        "  #     transforms.Resize(256),\n",
        "  #     transforms.CenterCrop(224),\n",
        "  #     transforms.ToTensor(),\n",
        "  #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  # ])\n",
        "  preprocess = lambda ims: torch.stack([transform(im.to_pil()) for im in ims]).to(device)\n",
        "  # A ResNet model to explain\n",
        "  # model = models.resnet50(pretrained=True).to(device)\n",
        "  # The postprocessing function is a simple softmax function transforming the output logits\n",
        "  # into class probabilities.\n",
        "  postprocess = lambda logits: torch.nn.functional.softmax(logits, dim=1)\n",
        "  # We now create a `VisionExplainer`, e.g., the selected explainers include Grad-CAM, LIME,\n",
        "  # integreated-gradient. `params` in `VisionExplainer` allows us to set parameters for each\n",
        "  # explainer applied here. For example, the target_layer for Grad-CAM#0 is set to the last\n",
        "  # layer of `model.layer4`.\n",
        "  explainer = VisionExplainer(\n",
        "      explainers=[\"lime\", \"ig\", \"gradcam#0\", \"gradcam#1\", \"gradcam#2\", \"gradcam#3\"],\n",
        "      mode=\"classification\",\n",
        "      model=model,\n",
        "      preprocess=preprocess,\n",
        "      postprocess=postprocess,\n",
        "      params={\n",
        "          \"gradcam#0\": {\"target_layer\": model.layer4[-1]},\n",
        "          \"gradcam#1\": {\"target_layer\": model.layer4[-2]},\n",
        "          \"gradcam#2\": {\"target_layer\": model.layer4[-1]},\n",
        "          \"gradcam#3\": {\"target_layer\": model.layer4[-2]},\n",
        "      }\n",
        "  )\n",
        "  # Generate explanations given the test instances. The label to explain for the first two\n",
        "  # Grad-CAM explainers is \"bull_mastiff\" (the top label) while the label for the second\n",
        "  # Grad-CAM explainers is \"tiger_cat\" (label = 281).\n",
        "  # compare gradcam for opposite class\n",
        "  local_explanations = explainer.explain(\n",
        "      img,\n",
        "      params={\n",
        "          \"gradcam#2\": {\"y\": [1-test['label_num'][idx]]},\n",
        "          \"gradcam#3\": {\"y\": [1-test['label_num'][idx]]},\n",
        "      }\n",
        "  )\n",
        "  # get predictions\n",
        "  predictions = local_explanations['predict'].get_explanations()\n",
        "  # write combined html\n",
        "  with open(outpath, 'w') as f:\n",
        "    for k in local_explanations.keys():\n",
        "      fig = local_explanations[k]._plotly_figure(index=0, class_names='CL')\n",
        "      f.write(fig.to_html(full_html=True, include_plotlyjs='cdn'))\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "quG038RrtN7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run gradcam\n",
        "Output:\n",
        "1. Write .html with plotly visualizations to google drive\n",
        "2. Save predictions and class probabilities in csv"
      ],
      "metadata": {
        "id": "wkvysUgv49le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "lab = ['CE', 'LAA']\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device=='cpu':\n",
        "  model = torch.load('/content/drive/MyDrive/XGD_explainable/230421_resnet18_mayo-clinic-tiled-2gb.pt',\n",
        "                   map_location=torch.device('cpu'))\n",
        "else:\n",
        "  model = torch.load('/content/drive/MyDrive/XGD_explainable/230421_resnet18_mayo-clinic-tiled-2gb.pt')\n",
        "test = pd.read_csv('/content/xgdiss/test.csv')\n",
        "test['prediction'] = np.zeros(test.shape[0]) \n",
        "test['prob_CE'] = np.zeros(test.shape[0]) \n",
        "test['prob_LAA'] = np.zeros(test.shape[0]) \n",
        "test = test.iloc[:,1:] # drop unnamed: 0 col\n",
        "### index of test ###\n",
        "# idx = 0\n",
        "# label = test['label'][idx]\n",
        "# outpath = f'/content/drive/MyDrive/XGD_explainable/html/{label}/plotly-test-idx_{idx}-{label}.html'"
      ],
      "metadata": {
        "id": "B_y8Ah5YT43G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOOP FOR ALL TEST SET ###\n",
        "## Save \n",
        "num_imgs = 10\n",
        "# num_imgs = test.shape[0] # ALL TEST IMAGES\n",
        "n = 50 # every 50 images, save test\n",
        "for idx in range(num_imgs):\n",
        "  print(f'Working on {idx}')\n",
        "  label = test['label'][idx]\n",
        "  outpath = f'/content/drive/MyDrive/XGD_explainable/html/{label}/plotly-test-idx_{idx}-{label}.html'\n",
        "  predictions = plot_gradcam(model, test, idx, outpath)\n",
        "  prob = predictions['values'][0]\n",
        "  prob_CE = prob[0]\n",
        "  prob_LAA = prob[1]\n",
        "  pred = lab[np.argmax(prob)]\n",
        "  test.loc[idx,'prediction'] = pred\n",
        "  test.loc[idx,'prob_CE'] = prob_CE\n",
        "  test.loc[idx,'prob_LAA'] = prob_LAA\n",
        "  if idx % 50 == 0:\n",
        "    test.to_csv(f'/content/drive/MyDrive/XGD_explainable/html/test-{idx}.csv',\n",
        "            index=False)"
      ],
      "metadata": {
        "id": "HqcgN-MreQYV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}